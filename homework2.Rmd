---
title: "Homework 2"
author: "Jamie McGraw"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false


library(tidyverse)
library(wbstats)
library(skimr)
library(countrycode)
library(here)
```

# Data Visualisation - Exploration

Now that you've demonstrated your software is setup, and you have the basics of data manipulation, the goal of this assignment is to practice transforming, visualising, and exploring data.

# Mass shootings in the US

In July 2012, in the aftermath of a mass shooting in a movie theater in Aurora, Colorado, [Mother Jones](https://www.motherjones.com/politics/2012/07/mass-shootings-map/) published a report on mass shootings in the United States since 1982. Importantly, they provided the underlying data set as [an open-source database](https://www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/) for anyone interested in studying and understanding this criminal behavior.

## Obtain the data

```{r}
#| echo: false
#| message: false
#| warning: false

mass_shootings <- read_csv(here::here("data", "mass_shootings.csv"))

glimpse(mass_shootings)
```

| column(variable)     | description                                                                 |
|--------------------------|----------------------------------------------|
| case                 | short name of incident                                                      |
| year, month, day     | year, month, day in which the shooting occurred                             |
| location             | city and state where the shooting occcurred                                 |
| summary              | brief description of the incident                                           |
| fatalities           | Number of fatalities in the incident, excluding the shooter                 |
| injured              | Number of injured, non-fatal victims in the incident, excluding the shooter |
| total_victims        | number of total victims in the incident, excluding the shooter              |
| location_type        | generic location in which the shooting took place                           |
| male                 | logical value, indicating whether the shooter was male                      |
| age_of_shooter       | age of the shooter when the incident occured                                |
| race                 | race of the shooter                                                         |
| prior_mental_illness | did the shooter show evidence of mental illness prior to the incident?      |

## Explore the data

### Specific questions

-   Generate a data frame that summarizes the number of mass shootings per year.

```{r}

yearly_shootings <- mass_shootings %>% 
  group_by(year) %>% 
  summarise(count = n())
```

-   Generate a bar chart that identifies the number of mass shooters associated with each race category. The bars should be sorted from highest to lowest and each bar should show its number.

```{r}

#first let's count race 
race_counts <- mass_shootings %>% 
  na.omit() %>% 
  group_by(race) %>% 
  summarise(count = n()) %>% 
  arrange(-count)

print(race_counts)
#let's then plot this information, by far the majority are White
ggplot(race_counts, aes(x = reorder(race, -count), y = count)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  labs(x = "Race", y = "Number of Shooters", title = "Number of Mass Shooters by race") + 
  theme_minimal()


```

-   Generate a boxplot visualizing the number of total victims, by type of location.

```{r}
ggplot(mass_shootings, aes(x = location_type, y = total_victims)) +
  geom_boxplot(fill = "steelblue") +
  labs(x = "Type of Location", y = "Total Victims", title = "Total Victims by Type of Location") +
  theme_minimal()
```

-   Redraw the same plot, but remove the Las Vegas Strip massacre from the dataset.

```{r}
#evidently there is a huge outlier which is the Las Vegas Strip massacre so let's just exclude that datapoint manually
no_outlier_shootings <- mass_shootings %>% 
  filter(total_victims <200)

ggplot(no_outlier_shootings, aes(x = location_type, y = total_victims)) +
  geom_boxplot(fill = "steelblue") +
  labs(x = "Type of Location", y = "Total Victims", title = "Total Victims by Type of Location") +
  theme_minimal()
```

### More open-ended questions

Address the following questions. Generate appropriate figures/tables to support your conclusions.

-   How many white males with prior signs of mental illness initiated a mass shooting after 2000?

```{r}

#let's filter the data to match the criteria and then count the number of rows

mental_illness_shootings <- mass_shootings %>%
  filter(prior_mental_illness == "Yes", year > 2000, race == "White", male == TRUE) %>% 
  summarise(count = n())

print(mental_illness_shootings)

```

-   Which month of the year has the most mass shootings? Generate a bar chart sorted in chronological (natural) order (Jan-Feb-Mar- etc) to provide evidence of your answer.

```{r}


#first count the number of shootings each month chronogically
danger_month <- mass_shootings %>% 
  group_by(month) %>% 
  summarise(count = n())

#probably there is a more elegant way to do it but I'm going to manually label each month's order and add a column to the dataframe then reorder
danger_month <- danger_month %>% 
  mutate(month_order = case_when(
    month =="Jan" ~"1",
    month =="Feb" ~"2",
    month =="Mar" ~"3",
    month =="Apr" ~"4",
    month =="May" ~"5",
    month =="Jun" ~"6", 
    month =="Jul" ~"7",
    month =="Aug" ~"8", 
    month =="Sep" ~"9", 
    month =="Oct" ~"10",
    month =="Nov" ~"11", 
    month =="Dec" ~"12",
    TRUE ~ ""
  ))
#new column is going to be a string so let's convert it to a number
danger_month <- danger_month %>%  
  mutate(month_order = as.numeric(month_order))

#now we can reorder it
danger_month <- danger_month %>% 
  arrange(month_order)
#check that the ordering is now correct --> yes it is. 
print(danger_month)

#now we print ggplot, looks like February has the most, but not by much, especially this is over 30 years.
ggplot(danger_month, aes(x = reorder(month, month_order) , y = count)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  labs(x = "Month", y = "Number of Shootings", title = "Number of Shootings by Month") + 
  theme_minimal()
```

-   How does the distribution of mass shooting fatalities differ between White and Black shooters? What about White and Latino shooters?

```{r}
str(mass_shootings)
#we'll just tell ggplot to fiter within ggpot
fatalities_white <- ggplot(mass_shootings[mass_shootings$race == "White",], aes(x = fatalities)) +
  geom_histogram(fill = "steelblue", color = "white") + 
  labs(
    x = "Fatalities per Shooting",
    y = "Fatality frequency",
    title = "Histogram of Fatalities by White"
    ) + 
  xlim(0,120)

fatalities_black <- ggplot(mass_shootings[mass_shootings$race == "Black",], aes(x = fatalities)) +
  geom_histogram(fill = "steelblue", color = "white") + 
  labs(
    x = "Fatalities per Shooting",
    y = "Fatality frequency",
    title = "Histogram of Fatalities by Black"
    ) + 
  xlim(0,15)

#let's look at the graphs, black have far lower frequency, and lower fatalities per shooting
print(fatalities_white)
print(fatalities_black)

```

### Very open-ended

-   Are mass shootings with shooters suffering from mental illness different from mass shootings with no signs of mental illness in the shooter?

```{r}
#remind ourselves how the database is organised
str(mass_shootings)

#OK I'm just going to look at quantitative data and practice overlaying histograms at the same time. 

#let's try create an overlay of distributions along a few variables. First let's save the filtered df's with mental illness vs no mental illness

mentally_ill_shooters <- mass_shootings %>% 
  filter(prior_mental_illness == "Yes")


mentally_healthy_shooters <- mass_shootings %>% 
  filter(prior_mental_illness == "No")

#let's first see how fatalities differ: there are fewer "healthy" shooters and there are fewer shootings which have double digit fatalities
ggplot() +
  geom_histogram(data = mentally_ill_shooters, aes(x = fatalities), fill = "steelblue", alpha = 0.5, binwidth = 1) +
  geom_histogram(data = mentally_healthy_shooters, aes(x = fatalities), fill = "red", alpha = 0.5, binwidth = 1) +
  labs(
    x = "Fatalities per shootings",
    y = "Frequency",
    title = "Overlay of fatality histograms"
  ) +
  xlim(0, 120)

#let's look at injured next: again fewer injuries and fewer high double digit injuries so in general we can generalise that mentally ill shootings are more common and more deadly
ggplot() +
  geom_histogram(data = mentally_ill_shooters, aes(x = injured), fill = "steelblue", alpha = 0.5, binwidth = 1) +
  geom_histogram(data = mentally_healthy_shooters, aes(x = injured), fill = "red", alpha = 0.5, binwidth = 1) +
  labs(
    x = "Injuries per shooting",
    y = "Frequency",
    title = "Overlay of injury histograms"
  ) +
  xlim(0, 120)

#finally let's look at the age --> no obvious differences
ggplot() +
  geom_histogram(data = mentally_ill_shooters, aes(x = age_of_shooter), fill = "steelblue", alpha = 0.5, binwidth = 1) +
  geom_histogram(data = mentally_healthy_shooters, aes(x = age_of_shooter), fill = "red", alpha = 0.5, binwidth = 1) +
  labs(
    x = "Age",
    y = "Frequency",
    title = "Overlay of injury histograms"
  ) +
  xlim(0, 120)

```

-   Assess the relationship between mental illness and total victims, mental illness and location type, and the intersection of all three variables.

```{r}
#we're just going to visually inspect this: visually, we can see there are more shootings (dots) with prior mentally ill shooters,
#we can see there are more shootings with high number of victims
#we can see thre are relatively more shootings in the "other" location types for non-ill shooters, suggesting more predictability by place
# we can see religious airport and military settings are relatively uncommon, we can see that school shootings are more dangerous on average than workplace settings
# we can see that non-mentally ill shooters are relatively more concentrated in schools than in other location types (apart from "other") 
ggplot(mass_shootings, aes(x = total_victims, y = location_type, color = prior_mental_illness)) + 
  geom_point(data = na.omit(mass_shootings)) + 
  labs(
    x = "total victims",
    y = "prior mental illness",
    title = "total victims, mental illness and location type"
  ) +
  xlim(0,120)

```

Make sure to provide a couple of sentences of written interpretation of your tables/figures. Graphs and tables alone will not be sufficient to answer this question.

# Exploring credit card fraud

We will be using a dataset with credit card transactions containing legitimate and fraud transactions. Fraud is typically well below 1% of all transactions, so a naive model that predicts that all transactions are legitimate and not fraudulent would have an accuracy of well over 99%-- pretty good, no? (well, not quite as we will see later in the course)

You can read more on credit card fraud on [Credit Card Fraud Detection Using Weighted Support Vector Machine](https://www.scirp.org/journal/paperinformation.aspx?paperid=105944)

The dataset we will use consists of credit card transactions and it includes information about each transaction including customer details, the merchant and category of purchase, and whether or not the transaction was a fraud.

## Obtain the data

The dataset is too large to be hosted on Canvas or Github, so please download it from dropbox https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0 and save it in your `dsb` repo, under the `data` folder

```{r}
#| echo: false
#| message: false
#| warning: false

card_fraud <- read_csv(here::here("data", "card_fraud.csv"))

glimpse(card_fraud)
```

The data dictionary is as follows

| column(variable)      | description                                 |
|-----------------------|---------------------------------------------|
| trans_date_trans_time | Transaction DateTime                        |
| trans_year            | Transaction year                            |
| category              | category of merchant                        |
| amt                   | amount of transaction                       |
| city                  | City of card holder                         |
| state                 | State of card holder                        |
| lat                   | Latitude location of purchase               |
| long                  | Longitude location of purchase              |
| city_pop              | card holder's city population               |
| job                   | job of card holder                          |
| dob                   | date of birth of card holder                |
| merch_lat             | Latitude Location of Merchant               |
| merch_long            | Longitude Location of Merchant              |
| is_fraud              | Whether Transaction is Fraud (1) or Not (0) |

-   In this dataset, how likely are fraudulent transactions? Generate a table that summarizes the number and frequency of fraudulent transactions per year.

```{r}

fraud_frequency <- card_fraud %>% 
  group_by(is_fraud) %>% 
  summarise(count = n())
#not sure how to efficiently add the column in one go so I'll do it in a few steps, starting by summing the count
fraud_count <- sum(fraud_frequency$count)
#then create a column that inserts the % frequency
fraud_frequency <- fraud_frequency %>% 
  mutate(percent_frequency = ifelse (is_fraud == 1,100*count/fraud_count,100*count/fraud_count))
#check the value: so fraud happen in just over half a percent of transcations
print(fraud_frequency)

```

-   How much money (in US\$ terms) are fraudulent transactions costing the company? Generate a table that summarizes the total amount of legitimate and fraudulent transactions per year and calculate the % of fraudulent transactions, in US\$ terms.

```{r}
#let's create the fraud indicator first
fraud_indicator <- card_fraud %>% 
  group_by(is_fraud) 
#then create a total transaction value amount
summary_data <- fraud_indicator %>% 
  summarise(total_amount = sum(amt))

#not sure how to efficiently add the column in one go so I'll do it in a few steps, starting by summing the transaction value
total_amount_both <- sum(summary_data$total_amount)
#then create a column that inserts the % frequency
summary_data <- summary_data %>% 
  mutate(percent_frequency = ifelse (is_fraud == 1,100*total_amount/total_amount_both,100*total_amount/total_amount_both))
#check the value: so even though fraud only accounts for 0.5% of cases, it accounts for 4.39 of total transaction value a 10x increase!
print(summary_data)

```

-   Generate a histogram that shows the distribution of amounts charged to credit card, both for legitimate and fraudulent accounts. Also, for both types of transactions, calculate some quick summary statistics.

```{r}
#first let's store the data for different fraud states
is_fraud_histo <- card_fraud %>% 
  filter(is_fraud == 1)

is_not_fraud_histo <- card_fraud %>% 
  filter(is_fraud == 0)
#then let's overlay the histograms so we can compare directly, actually to do this effectively I'll use density plot instead
ggplot() +
  geom_density(data = is_fraud_histo, aes(x = amt), fill = "steelblue", alpha = 0.5) +
  geom_density(data = is_not_fraud_histo, aes(x = amt), fill = "red", alpha = 0.5) +
  labs(
    x = "Transaction value",
    y = "Frequency",
    title = "Overlay of transaction value by fraudulent and non-fradulent transactions"
  ) +
  xlim(0,1500)

#summary stats for fraud
mean(is_fraud_histo$amt)
median(is_fraud_histo$amt)
sd(is_fraud_histo$amt)
min(is_fraud_histo$amt)
max(is_fraud_histo$amt)
#summary stats for not fraud
mean(is_not_fraud_histo$amt)
median(is_not_fraud_histo$amt)
sd(is_not_fraud_histo$amt)
min(is_not_fraud_histo$amt)
max(is_not_fraud_histo$amt)


```

-   What types of purchases are most likely to be instances of fraud? Consider category of merchants and produce a bar chart that shows % of total fraudulent transactions sorted in order.

```{r}
#create the table first
fraud_by_category <- card_fraud %>%
  group_by(category) %>%
  summarize(fraud_percentage = sum(is_fraud == 1) /n()) %>%
  mutate(fraud_percentage = fraud_percentage * 100) %>%
  arrange(desc(fraud_percentage))
#reorder it
fraud_by_category <- fraud_by_category %>%
  mutate(category = reorder(category, -fraud_percentage))

# Create a bar chart showing the percentages of total fraudulent transactions
ggplot(data = fraud_by_category, aes(x = category,  y = fraud_percentage, fill = category)) +
  geom_bar(stat = "identity") +
  xlab("Category of Merchants") +
  ylab("% of Total Fraudulent Transactions") +
  ggtitle("Fraudulent Transactions by Category of Merchants") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_discrete(name = "Category of Merchants")


```

-   When is fraud more prevalent? Which days, months, hours? To create new variables to help you in your analysis, we use the `lubridate` package and the following code

```         
mutate(
  date_only = lubridate::date(trans_date_trans_time),
  month_name = lubridate::month(trans_date_trans_time, label=TRUE),
  hour = lubridate::hour(trans_date_trans_time),
  weekday = lubridate::wday(trans_date_trans_time, label = TRUE)
  )
```

-   Are older customers significantly more likely to be victims of credit card fraud? To calculate a customer's age, we use the `lubridate` package and the following code

```         
  mutate(
   age = interval(dob, trans_date_trans_time) / years(1),
    )
```

```{r}

library(lubridate)

# Create new variables for analysis
card_fraud <- card_fraud %>%
  mutate(
    date_only = date(trans_date_trans_time),
    month_name = month(trans_date_trans_time, label = TRUE),
    hour = hour(trans_date_trans_time),
    weekday = wday(trans_date_trans_time, label = TRUE)
  )

fraud_by_weekday <- card_fraud %>%
  group_by(weekday) %>%
  summarize(fraud_count = sum(is_fraud == 1), total_count = n()) %>%
  mutate(fraud_percentage = fraud_count / total_count * 100)

# Sort the weekdays in the correct order
fraud_by_weekday$weekday <- factor(fraud_by_weekday$weekday, levels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"))

# Create a bar chart to visualize the prevalence of fraud by weekday, thursday much more likely than sunday (about 40% more common)
ggplot(data = fraud_by_weekday, aes(x = weekday, y = fraud_percentage, fill = weekday)) +
  geom_bar(stat = "identity") +
  xlab("Weekday") +
  ylab("% of Fraudulent Transactions") +
  ggtitle("Prevalence of Fraud by Weekday")

#now by month
fraud_by_month <- card_fraud %>%
  group_by(month_name) %>%
  summarize(fraud_count = sum(is_fraud == 1), total_count = n()) %>%
  mutate(fraud_percentage = fraud_count / total_count * 100)

# Create a bar chart to visualize the prevalence of fraud by month, Jan and Feb ~50% more frequent than July or December
ggplot(data = fraud_by_month, aes(x = month_name, y = fraud_percentage, fill = month_name)) +
  geom_bar(stat = "identity") +
  xlab("Month") +
  ylab("% of Fraudulent Transactions") +
  ggtitle("Prevance of Fraud by Month")

#now by hour
fraud_by_hour <- card_fraud %>%
  group_by(hour) %>%
  summarize(fraud_count = sum(is_fraud == 1), total_count = n()) %>%
  mutate(fraud_percentage = fraud_count / total_count * 100)

# Create a line plot to visualize the prevalence of fraud by hour, far more likely to happen late in the evening 10 - 30 times more likely.  
ggplot(data = fraud_by_hour, aes(x = hour, y = fraud_percentage)) +
  geom_bar(stat = "identity") +
  xlab("Hour") +
  ylab("% of Fraudulent Transactions") +
  ggtitle("Prevalence of Fraud by Hour")



```

-   Is fraud related to distance? The distance between a card holder's home and the location of the transaction can be a feature that is related to fraud. To calculate distance, we need the latidue/longitude of card holders's home and the latitude/longitude of the transaction, and we will use the [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula) to calculate distance. I adapted code to [calculate distance between two points on earth](https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/) which you can find below

```{r}
# distance between card holder's home and transaction
# code adapted from https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/


card_fraud <- card_fraud %>%
  mutate(
    
    # convert latitude/longitude to radians
    lat1_radians = lat / 57.29577951,
    lat2_radians = merch_lat / 57.29577951,
    long1_radians = long / 57.29577951,
    long2_radians = merch_long / 57.29577951,
    
    # calculate distance in miles
    distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)),

    # calculate distance in km
    distance_km = 6377.830272 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians))

  )

#plot the boxplot
ggplot(data = card_fraud, aes(x = is_fraud, y = distance_km)) +
  geom_boxplot() +
  xlab("Fraud Indicator") +
  ylab("Distance (km)") +
  ggtitle("Relationship between Distance and Fraud") +
  theme_minimal()

```

Plot a boxplot or a violin plot that looks at the relationship of distance and `is_fraud`. Does distance seem to be a useful feature in explaining fraud?

# Exploring sources of electricity production, CO2 emissions, and GDP per capita.

There are many sources of data on how countries generate their electricity and their CO2 emissions. I would like you to create three graphs:

## 1. A stacked area chart that shows how your own country generated its electricity since 2000.

You will use

`geom_area(colour="grey90", alpha = 0.5, position = "fill")`

## 2. A scatter plot that looks at how CO2 per capita and GDP per capita are related

## 3. A scatter plot that looks at how electricity usage (kWh) per capita/day GDP per capita are related

We will get energy data from the Our World in Data website, and CO2 and GDP per capita emissions from the World Bank, using the `wbstats`package.

```{r}
#| message: false
#| warning: false
rm(list = ls())
# Download electricity data
url <- "https://nyc3.digitaloceanspaces.com/owid-public/data/energy/owid-energy-data.csv"

energy <- read_csv(url) %>% 
  filter(year >= 1990) %>% 
  drop_na(iso_code) %>% 
  select(1:3,
         biofuel = biofuel_electricity,
         coal = coal_electricity,
         gas = gas_electricity,
         hydro = hydro_electricity,
         nuclear = nuclear_electricity,
         oil = oil_electricity,
         other_renewable = other_renewable_exc_biofuel_electricity,
         solar = solar_electricity,
         wind = wind_electricity, 
         electricity_demand,
         electricity_generation,
         net_elec_imports,	# Net electricity imports, measured in terawatt-hours
         energy_per_capita,	# Primary energy consumption per capita, measured in kilowatt-hours	Calculated by Our World in Data based on BP Statistical Review of World Energy and EIA International Energy Data
         energy_per_gdp,	# Energy consumption per unit of GDP. This is measured in kilowatt-hours per 2011 international-$.
         per_capita_electricity, #	Electricity generation per capita, measured in kilowatt-hours
  ) 

# Download data for C02 emissions per capita https://data.worldbank.org/indicator/EN.ATM.CO2E.PC
co2_percap <- wb_data(country = "countries_only", 
                      indicator = "EN.ATM.CO2E.PC", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         co2percap = value)

############################################################################################
############################################################################################


# Download data for GDP per capita  https://data.worldbank.org/indicator/NY.GDP.PCAP.PP.KD
gdp_percap <- wb_data(country = "countries_only", 
                      indicator = "NY.GDP.PCAP.PP.KD", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         GDPpercap = value)


#convert to long, tidy format by shifting the energy sources into one variable and their respective values into another
energy_tidy <- energy %>% 
  pivot_longer(cols = c(4,5,6,7,8,9,10,11,12), 
               names_to = "energy_source",
               values_to = "energy_use")

#make sure ISO codes column names match
colnames(gdp_percap)[4] <- "iso_code"
colnames(co2_percap)[4] <- "iso_code"
#join the energy_tidy and gdp_percap
merged_data <- left_join(gdp_percap, energy_tidy, by = c("iso_code","year"))

#join co2_percap to the merged data
merged_data <- left_join(merged_data, co2_percap, by = c('iso_code', "year"))

#example code given
knitr::include_graphics(here::here("images", "electricity-co2-gdp.png"), error = FALSE)

#############Question 1################
#######################################
#Question 1: let's first try electricity generation stacked area chart for South Africa --> as expected majority coal

zaf_energy <- merged_data %>% 
  filter(iso_code == "ZAF")

ggplot(zaf_energy, aes(x = year, y = energy_use, fill = energy_source)) + 
  geom_area(colour="grey90", alpha = 0.5, position = "fill") 

#give South Africa's electricity crisis, I was curious to see whether the increase share in renewables is attributable to dropping output overall, 
# as we can see from the below, energy output overall has declined since 2010 :( 
ggplot(zaf_energy, aes(x = year, y = energy_use, fill = energy_source)) + 
  geom_bar(stat = "identity") 

#############Question 2################
#######################################

#playing around with formatting to see what we can map in a 2d space, e.g. it seems like the relationship between GDP per capita and co2 is getting weaker over time
#i.e. hotter colours show a steeper slope than colder colours. the triangles and circles are probably overkill
ggplot(merged_data, aes(x = GDPpercap, y = co2percap, color = year))+
  geom_point(size = 0.5, aes(shape = ifelse(GDPpercap > 50000, "circle", "triangle")))+
  labs(x ="GDP per capita",
       y = "CO2 per capita",
       title = "CO2 vs GDP (per capita)",
       shape = "GDP high (circle) vs low (triangle)")+
  scale_color_gradientn(colours = rainbow(10))+
  scale_shape_manual(values = c("circle"=1, "triangle"=2))


#############Question 3################
#######################################
#let's look at KwH and GDP per capita


```

Specific questions:

1.  How would you turn `energy` to long, tidy format?
2.  You may need to join these data frames
    -   Use `left_join` from `dplyr` to [join the tables](http://r4ds.had.co.nz/relational-data.html)
    -   To complete the merge, you need a unique *key* to match observations between the data frames. Country names may not be consistent among the three dataframes, so please use the 3-digit ISO code for each country
    -   An aside: There is a great package called [`countrycode`](https://github.com/vincentarelbundock/countrycode) that helps solve the problem of inconsistent country names (Is it UK? United Kingdom? Great Britain?). `countrycode()` takes as an input a country's name in a specific format and outputs it using whatever format you specify.
3.  Write a function that takes as input any country's name and returns all three graphs. You can use the `patchwork` package to arrange the three graphs as shown below

```{r, echo=FALSE, out.width="100%"}






  ```


# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown (qmd) file as a Word or HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas. You must be comitting and pushing your changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: No 
-   Approximately how much time did you spend on this problem set: about 8 hours total
-   What, if anything, gave you the most trouble: the last question... didn't have time for it. 

**Please seek out help when you need it,** and remember the [15-minute rule](https://dsb2023.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
